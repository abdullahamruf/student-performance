# -*- coding: utf-8 -*-
"""hsc_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t8SY8ORMFDphjqmKL1jpTApyMdNXq66u
"""

import pandas as pd
df=pd.read_csv('/content/new.csv')
df = df.drop('29/04/2023', axis=1)
new_column_names = {
    'gender\nstudent\'s sex (binary: \'F\' - female or \'M\' - male)': 'gender',
}

# Rename the columns
df = df.rename(columns=new_column_names)
new_column_names = {
    'gender\nstudent\'s sex (binary: \'F\' - female or \'M\' - male)': 'gender',
    "age\nstudent's age (numeric: from 15 to 22)": 'age',
    # Add other column name mappings here
}

# Rename the columns
df = df.rename(columns=new_column_names)
new_column_names = {
    'gender\nstudent\'s sex (binary: \'F\' - female or \'M\' - male)': 'gender',
    "age\nstudent's age (numeric: from 15 to 22)": 'age',
    "Medu\nmother's education (numeric: 0 - none, 1 - primary education (till 4th grade), 2 - (5th to 9th grade), 3 - (secondary), 4 - (higher)": 'mother_education',
    "Fedu\nfather's education (numeric: 0 - none, 1 - primary education (till 4th grade), 2 - (5th to 9th grade), 3 - (secondary), 4 - (higher)": 'father_education',
    "adress\nstudent's home address type (binary: 'U' - urban or 'R' - rural)": 'address_type',
    "famsize\nfamily size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)": 'family_size',
    "Pstatus\nparent's cohabitation status (binary: 'T' - living together or 'A' - apart)": 'cohabitation_status',
    "Medu\nmother's education (numeric: 0 - none, 1 - primary education (till 4th grade), 2 - (5th to 9th grade), 3 - (secondary), 4 - (higher)": 'mother_education',
    "Fedu\nfather's education (numeric: 0 - none, 1 - primary education (till 4th grade), 2 - (5th to 9th grade), 3 - (secondary), 4 - (higher)": 'father_education',
    "Mjob\nmother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or other": 'mother_job',
    "Fjob\nfather's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or other": 'father_job',
    # Add other column name mappings here
}

# Rename the columns
df = df.rename(columns=new_column_names)
# Convert gender to binary (0 for 'F', 1 for 'M')
df['gender'] = df['gender'].map({'F': 0, 'M': 1})

# Convert address type to binary (0 for 'U', 1 for 'R')
df['address_type'] = df['address_type'].map({'U': 0, 'R': 1})


df['cohabitation_status'] = df['cohabitation_status'].str.strip().replace({'T': 0, 'A': 1}).astype(int)
mapping = {
    'at_home': 0,
    'other': 1,
    'teacher': 2,
    'services': 3,
    'health': 4
}

df['father_job'] = df['father_job'].str.strip().replace(mapping)
mapping = {
    'at_home': 0,
    'other': 1,
    'teacher': 2,
    'services': 3,
    'health': 4
}

df['mother_job'] = df['mother_job'].str.strip().replace(mapping)
df['mother_job'].value_counts().astype(int)
mapping = {'yes': 0, 'No': 1, 'Yes': 0, 'YES': 0}
df['relationship_breakdown'] = df['relationship_breakdown'].str.strip().replace(mapping).astype(int)

df['smoker'] = df['smoker'].str.strip().replace({'No': 1, 'yes': 0, 'Yes': 0}).astype(int)
df['family_size'] = df['family_size'].str.strip().replace({'GT3': 1, 'LE3': 0, }).astype(int)


df['time_spent_with_friends'] = df['time_spent_with_friends'].str.strip().replace({'Not too much': 1}).astype(int)

def convert_to_grade(result):
    if result >= 1.0 and result <= 2.0:
        return "1"
    elif result > 2.0 and result <= 3.0:
        return "2"
    elif result > 3.0 and result <= 3.99:
        return "3"
    elif result >= 4.0 and result <= 4.50:
        return "4"
    elif result >= 4.51 and result <= 4.99:
        return "4.50"
    elif result == 5.0:
        return "5"
    else:
        return "Unknown"

# Apply the conversion function to the 'ssc_result' column
df['ssc_result'] = df['ssc_result'].apply(convert_to_grade)

# Apply the conversion function to the 'hsc_result' column
df['hsc_result'] = df['hsc_result'].apply(convert_to_grade)


# Print the updated dataset
print(df)

new_dataset = df.drop('avarage_tution_fee_cost', axis=1)
new_dataset.head()

new_dataset = new_dataset.drop('ssc_result', axis=1)
new_dataset.head()

#one-hot encoding
categorical_cols = ['gender', 'address_type', 'cohabitation_status', 'M_Education', 'F_education', 'mother_job', 'father_job', 'relationship_breakdown', 'smoker']
df_encoded = pd.get_dummies(df, columns=categorical_cols)
X = df_encoded.drop(['hsc_result'], axis=1)
y = df_encoded[['hsc_result']]
print("Encoded DataFrame:")
print(df_encoded)

#label encoder
from sklearn.preprocessing import LabelEncoder
categorical_cols = ['gender', 'address_type', 'cohabitation_status', 'M_Education', 'F_education', 'mother_job', 'father_job', 'relationship_breakdown', 'smoker']
label_encoder = LabelEncoder()
for col in categorical_cols:
    df[col] = label_encoder.fit_transform(df[col])
X = new_dataset.drop(['hsc_result'], axis=1)
y = new_dataset[['hsc_result']]
print("Encoded DataFrame:")
print(new_dataset)

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import skew
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, PowerTransformer, QuantileTransformer, Normalizer, Binarizer
from sklearn.linear_model import LogisticRegression, SGDClassifier, ElasticNet, RidgeClassifier, Perceptron
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.tree import DecisionTreeClassifier
def plot_correlation_heatmap(data):
    correlation = data.corr()  # Calculate the correlation matrix of the data

    # Set the size of the figure for the heatmap
    plt.figure(figsize=(20, 16))

    # Create the heatmap with correlation values and annotations
    sns.heatmap(correlation, cmap='BrBG', annot=True)

    # Set the title of the heatmap
    plt.title('Correlation Heatmap')

    # Display the heatmap
    plt.show()
plot_correlation_heatmap(new_dataset)

# data_variable=new_dataset.drop(columns='hsc_result')
# X = data_variable.drop(columns=['ssc_result'])
# Y = new_dataset['ssc_result']
import warnings
import logging
warnings.filterwarnings("ignore")
random_seed = 42
np.random.seed(random_seed)
# Random oversamplingrandom_state=random_seed
oversampler = RandomOverSampler(random_state=random_seed)
X_resampled, Y_resampled = oversampler.fit_resample(X,y)
x_train, x_test, y_train, y_test = train_test_split(X_resampled,Y_resampled,test_size=0.2,random_state=random_seed)

import joblib
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import skew
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, PowerTransformer, QuantileTransformer, Normalizer, Binarizer
from sklearn.linear_model import LogisticRegression, SGDClassifier, ElasticNet, RidgeClassifier, Perceptron
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.mixture import GaussianMixture
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report
from sklearn.preprocessing import LabelEncoder
import warnings
import logging
warnings.filterwarnings("ignore")
random_seed = 42
np.random.seed(random_seed)

from sklearn.preprocessing import LabelEncoder

# Convert the target variable to integer representation
label_encoder = LabelEncoder()
Y_resampled_encoded = label_encoder.fit_transform(Y_resampled)

new_dataset

Y_resampled.value_counts()

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Boosting classifiers
adaboost_clf = AdaBoostClassifier()
gradientboost_clf = GradientBoostingClassifier()

# Bagging meta-model
bagging_meta_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Stacking ensemble model
stacking_ensemble = StackingClassifier(
    estimators=[('adaboost', adaboost_clf), ('gradientboost', gradientboost_clf)],
    final_estimator=bagging_meta_model
)

from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, PowerTransformer, QuantileTransformer, Normalizer, Binarizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

# Convert the target variable to integer representation
label_encoder = LabelEncoder()
Y_resampled_encoded = label_encoder.fit_transform(Y_resampled)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, Y_resampled_encoded, test_size=0.2, random_state=random_seed)

# Define the base models
base_models = [
    ('rf', RandomForestClassifier()),
    ('gb', GradientBoostingClassifier())
]

# Define the meta-model
meta_model = RandomForestClassifier()

# Create the stacking ensemble
stacking_ensemble = StackingClassifier(estimators=base_models, final_estimator=meta_model)

# Train the stacking ensemble on the training data
stacking_ensemble.fit(X_train, y_train)

# Make predictions on the test set using the stacking ensemble
y_pred = stacking_ensemble.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred, average='weighted')
precision = precision_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Test accuracy:", accuracy)
print("Test recall:", recall)
print("Test precision:", precision)
print("Test F1-score:", f1)

rf=RandomForestClassifier();

rf.fit(X_train, y_train)

# Make predictions on the test set using the stacking ensemble
y_pred = rf.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred, average='weighted')
precision = precision_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Test accuracy:", accuracy)
print("Test recall:", recall)
print("Test precision:", precision)
print("Test F1-score:", f1)

pip install lime

import lime

pip install shap

import shap

# compute SHAP values
# explainer = shap.KernelExplainer(rf)
# shap_values = explainer.shap_values(X)

shap_values_tree = shap.TreeExplainer(rf).shap_values(X_test[0:20])

shap.summary_plot(shap_values_tree, X_test[0:20])

shap_values_tree[0].shape

X.columns

explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X)

shap.summary_plot(shap_values, X.values, plot_type="bar", feature_names = X.columns)

class_names = ['5','4','4.5','3']

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier

# Assuming rf is a RandomForestClassifier or a similar model
# Make sure it supports predict_proba for probability estimates
y_score = rf.predict_proba(X_test)

# Binarize the labels
y_test_bin = label_binarize(y_test, classes=range(4))

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(4):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each class
plt.figure(figsize=(8, 6))

for i in range(4):
    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')

plt.plot(fpr["micro"], tpr["micro"], label='Micro-average (AUC = {0:0.2f})'.format(roc_auc["micro"]), linestyle='--', linewidth=2)

plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Multiclass Classification (HSC Result)')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import cohen_kappa_score, matthews_corrcoef

# Assuming y_test and y_pred are your true labels and predicted labels, respectively

# Calculate Cohen's Kappa
kappa = cohen_kappa_score(y_test, y_pred)
print("Cohen's Kappa:", kappa)

# Calculate Matthews Correlation Coefficient (MCC)
mcc = matthews_corrcoef(y_test, y_pred)
print("Matthews Correlation Coefficient:", mcc)

